{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound & velocity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Imports and data_folder access*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# switch the path to represent the correct directory when switching devices and fetch the latest repo from github\n",
    "# Marc_PC\n",
    "data_folder_1 = Path(r\"C:\\Users\\marc_\\OneDrive\\Documents\\Studie\\Master\\Master Internship 1 Neurophysics\\Project-collective-cortical-dynamics-with-non-linear-dimensionality-reduction\\data\")\n",
    "data_folder_2 = Path(r\"Z:\\davide\\2p_data\\441394_ribolla\")\n",
    "data_folder_3 = Path(r\"Z:\\davide\\2p_data\\441406_fiano\")\n",
    "\n",
    "# Amber_laptop\n",
    "#data_folder_1 = Path(r\"C:\\Users\\renek\\OneDrive\\Documenten\\Marc Studiemap\\Master Internship 1\\Project-collective-cortical-dynamics-with-non-linear-dimensionality-reduction\\data\")\n",
    "#data_folder_2 = Path(r\"Z:\\davide\\2p_data\\441394_ribolla\")\n",
    "#data_folder_3 = Path(r\"Z:\\davide\\2p_data\\441406_fiano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the data is on your machine.\n",
    "data_path_1 = data_folder_1.joinpath('20230211')\n",
    "\n",
    "# ribolla files\n",
    "data_path_2 = data_folder_2.joinpath('20230315')\n",
    "data_path_3 = data_folder_2.joinpath('20230324')\n",
    "data_path_4 = data_folder_2.joinpath('20230331')\n",
    "data_path_5 = data_folder_2.joinpath('20230404')\n",
    "data_path_6 = data_folder_2.joinpath('20230405')\n",
    "\n",
    "# fiano files\n",
    "data_path_7 = data_folder_3.joinpath('20230309')\n",
    "data_path_8 = data_folder_3.joinpath('20230317')\n",
    "data_path_9 = data_folder_3.joinpath('20230323')\n",
    "data_path_10 = data_folder_3.joinpath('20230328')\n",
    "data_path_11 = data_folder_3.joinpath('20230331')\n",
    "\n",
    "# Create a list to store all the data paths\n",
    "all_data_paths = [\n",
    "    data_path_1,\n",
    "    data_path_2,\n",
    "    data_path_3,\n",
    "    data_path_4,\n",
    "    data_path_5,\n",
    "    data_path_6,\n",
    "    data_path_7,\n",
    "    data_path_8,\n",
    "    data_path_9,\n",
    "    data_path_10,\n",
    "    data_path_11\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\behaviour_data.pickle',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\cnmf.hdf5',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\decoded_log.mat',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\metrics.pickle',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\neural_data.pickle',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\parameters.yml',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\tif_header.pickle',\n",
       " 'Z:\\\\davide\\\\2p_data\\\\441394_ribolla\\\\20230315\\\\trial_data.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src') # Add src folder to path.\n",
    "import file_management as fm # Import my file_management functions from /src.\n",
    "\n",
    "selected_data_session = all_data_paths[1] # select one of the sessions! 0-10\n",
    "fm.get_files_in_data_path(selected_data_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('Z:/davide/2p_data/441394_ribolla/20230315')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['behaviour_data.pickle', 'cnmf.hdf5', 'decoded_log.mat', 'metrics.pickle', 'neural_data.pickle', 'parameters.yml', 'tif_header.pickle', 'trial_data.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all the files in the selected data session\n",
    "files = os.listdir(selected_data_session)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') # Add src folder to path.\n",
    "\n",
    "import sound_velocity_analysis as sva # Import my sound velocity analysis functions from /src.\n",
    "import histogram_analysis as ha # Import my histogram analysis functions from /src."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Multi-Session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n",
      "The difference between the speed distributions before and after the sound onset is statistically significant (p < 0.0500)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dataframe to store the velocity results\n",
    "velocity_df = pd.DataFrame(columns=['Session', 'Average Velocity', 'Standard Deviation', 'Standard Error'])\n",
    "# Create an empty dataframe to store the t-test results\n",
    "ttest_df = pd.DataFrame(columns=['Session', 't-statistic', 'p-value'])\n",
    "\n",
    "for session in all_data_paths[1:]:\n",
    "    \n",
    "    trial_data_file = session.joinpath('trial_data.csv')\n",
    "    trial_data = pd.read_csv(trial_data_file)\n",
    "\n",
    "    bdata_file = session.joinpath('behaviour_data.pickle')\n",
    "    with open(bdata_file,'rb') as file:\n",
    "        b_data = pickle.load(file)\n",
    "\n",
    "    position = b_data['position']\n",
    "    pos_list = []\n",
    "    for i in range(len(trial_data)):\n",
    "        row = trial_data.iloc[i]\n",
    "        onset = row['env_onset'].astype(int)\n",
    "        offset = row['tunnel2_offset']\n",
    "\n",
    "        pos_segment = position[onset:offset]\n",
    "        #print(f'{i}: {np.max(pos_segment)-np.min(pos_segment)}')\n",
    "        # normalize\n",
    "        pos_segment = (pos_segment - np.min(pos_segment))/(np.max(pos_segment)-np.min(pos_segment))\n",
    "        pos_list.append(pos_segment)\n",
    "        position = b_data['position']\n",
    "        rz_onsets = []\n",
    "        tunnel1_onsets = []\n",
    "\n",
    "    for i in range(len(trial_data)):\n",
    "        row = trial_data.iloc[i]\n",
    "        onset = row['env_onset'].astype(int)\n",
    "        offset = row['tunnel2_offset']\n",
    "\n",
    "        pos_segment = position[onset:offset]\n",
    "        min_pos = np.min(pos_segment)\n",
    "        max_pos = np.max(pos_segment)\n",
    "\n",
    "        rz_pos = ha.compute_feature_position(row['reward_zone_onset'],position,min_pos,max_pos)\n",
    "        rz_onsets.append(rz_pos)\n",
    "\n",
    "        t1_pos = ha.compute_feature_position(row['tunnel1_onset'],position,min_pos,max_pos)\n",
    "        tunnel1_onsets.append(t1_pos)\n",
    "        \n",
    "        #normalize\n",
    "        pos_segment = (pos_segment - np.min(pos_segment))/(np.max(pos_segment)-np.min(pos_segment))\n",
    "\n",
    "        if i==0:\n",
    "            norm_pos = pos_segment\n",
    "        else:\n",
    "            norm_pos = np.hstack([norm_pos,pos_segment])\n",
    "    \n",
    "    vel_save_folder = Path('./imgs/velocity_plots')\n",
    "    vel_save_folder.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    vel = sva.compute_velocity(b_data, session, vel_save_folder, pos_sigma=2, vel_sigma=20, vel_win=10000, vel_smooth=100, show_plot=False) # plot 1\n",
    "    vel = vel[:len(norm_pos)]\n",
    "\n",
    "    vel_hist_save_folder = Path('./imgs/pos_vel_histograms')\n",
    "    vel_hist_save_folder.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    sva.pos_vel_histogram(norm_pos, vel, session, vel_hist_save_folder,  nbins=50, show_plot=False) # plot 2\n",
    "\n",
    "    # Define compute start & end.\n",
    "    t_on = 2000\n",
    "    t_off = 4000\n",
    "\n",
    "    # compute the vel_matrix\n",
    "    vel_matrix = sva.computed_sliced_matrix(trial_data, vel, 2000, 4000)\n",
    "\n",
    "    save_folder = Path('./imgs/average_velocity_plots')\n",
    "    save_folder.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    # Calculate average velocity, standard deviation, and standard error for this session\n",
    "    avg_vel, std_vel, sem_vel = sva.avg_std_sem_velocity(vel_matrix, t_on, t_off, session, save_folder, show_plot=False) # plot 3 + stats\n",
    "\n",
    "    # Create a temporary dataframe for the velocity results of the current session\n",
    "    session_vel_df = pd.DataFrame({\n",
    "        'Session': [str(session)],\n",
    "        'Average Velocity': [avg_vel],\n",
    "        'Standard Deviation': [std_vel],\n",
    "        'Standard Error': [sem_vel]\n",
    "    })\n",
    "    \n",
    "    # Append the temporary velocity dataframe to the velocity dataframe\n",
    "    velocity_df = pd.concat([velocity_df, session_vel_df], ignore_index=True)\n",
    "\n",
    "    t_stat, p_val = sva.ttest_speed_distribution(vel_matrix, 2000)\n",
    "\n",
    "    # Create a temporary dataframe for the t-test results of the current session\n",
    "    session_ttest_df = pd.DataFrame({\n",
    "        'Session': [str(session)],\n",
    "        't-statistic': [t_stat],\n",
    "        'p-value': [p_val]\n",
    "    })\n",
    "\n",
    "    # Append the temporary t-test dataframe to the t-test dataframe\n",
    "    ttest_df = pd.concat([ttest_df, session_ttest_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired path and filename for vel_df\n",
    "vel_df_output_path = Path('../data/final/velocity_stats.csv')\n",
    "\n",
    "# Save the velocity dataframe to a CSV file\n",
    "velocity_df.to_csv(vel_df_output_path, index=False)\n",
    "\n",
    "# Specify the desired path and filename for ttest_df\n",
    "ttest_df_output_path = Path('../data/final/vel_ttest_stats.csv')\n",
    "\n",
    "# Save the t-test dataframe to a CSV file\n",
    "ttest_df.to_csv(ttest_df_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
